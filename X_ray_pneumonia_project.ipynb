{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqwFViM-JRqQ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Donwload the df from kaggle"
      ],
      "metadata": {
        "id": "aVs3sx2zJtgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload()  # Upload the kaggle.json file"
      ],
      "metadata": {
        "id": "uHb27WtRJZHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "3parjNYSJZ8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Replace 'tolgadincer/labeled-chest-xray-images' with the dataset's Kaggle API link\n",
        "!kaggle datasets download -d tolgadincer/labeled-chest-xray-images\n",
        "\n",
        "# Unzip the downloaded dataset\n",
        "!unzip -q labeled-chest-xray-images.zip -d chest_xray_dataset"
      ],
      "metadata": {
        "id": "n6UxjG7DJaxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_normal = glob.glob('/content/chest_xray_dataset/chest_xray/train/NORMAL/*')\n",
        "train_pneumonia = glob.glob('/content/chest_xray_dataset/chest_xray/train/PNEUMONIA/*')\n",
        "\n",
        "test_normal = glob.glob('/content/chest_xray_dataset/chest_xray/test/NORMAL/*')\n",
        "test_pneumonia = glob.glob('/content/chest_xray_dataset/chest_xray/test/PNEUMONIA/*')"
      ],
      "metadata": {
        "id": "eUcTBeibJb5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_paths = train_normal + train_pneumonia\n",
        "train_labels = [0] * len(train_normal) + [1] * len(train_pneumonia)\n",
        "\n",
        "test_paths = test_normal + test_pneumonia\n",
        "test_labels = [0] * len(test_normal) + [1] * len(test_pneumonia)\n",
        "\n",
        "train_paths_split, val_paths, train_labels_split, val_labels = train_test_split(train_paths, train_labels, stratify=train_labels, test_size = 0.1, random_state=42)"
      ],
      "metadata": {
        "id": "MHlhcFkZJdU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create data loaders"
      ],
      "metadata": {
        "id": "Ro20-QS8Jv_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomXrayDataset(Dataset):\n",
        "    def __init__(self, labels, paths, transform=None, target_transform=None):\n",
        "        self.img_labels = labels\n",
        "        self.img_paths = paths\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        # Calculate class counts\n",
        "        self.class_counts = torch.bincount(torch.tensor(labels))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        image = Image.open(img_path).convert('L')\n",
        "        label = self.img_labels[idx]\n",
        "        label = torch.tensor(label)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "CKy1nmlWJfMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "bskSAJOIKIke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have already defined your CustomXrayDataset class and the necessary variables like train_labels, train_paths, etc.\n",
        "batch_size = 64\n",
        "# Define transformations\n",
        "resize_transform = transforms.Resize((64, 64))\n",
        "normalize = transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "\n",
        "# Combine the transformations into a single Compose object\n",
        "train_transform = transforms.Compose([\n",
        "    resize_transform,\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(180),  # Rotate images randomly up to 10 degrees\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "# Create train_dataset with the specified transformations\n",
        "train_dataset = CustomXrayDataset(labels=train_labels_split, paths=train_paths_split, transform=train_transform)\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = [1 / (train_dataset.class_counts[i] + 1e-6) for i in range(len(train_dataset.class_counts))]\n",
        "# Convert class weights to tensor\n",
        "class_weights_tensor = torch.tensor(class_weights)\n",
        "\n",
        "sample_weights = [class_weights_tensor[label] for label in train_dataset.img_labels]\n",
        "\n",
        "# Use WeightedRandomSampler to handle class imbalance\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(train_dataset), replacement=True)\n",
        "# Create DataLoader with the specified sampler\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)"
      ],
      "metadata": {
        "id": "0eJpEww9JhXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation and test"
      ],
      "metadata": {
        "id": "YhNg0wtmKEXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resize_transform = transforms.Resize((64, 64))\n",
        "normalize = transforms.Normalize(mean=[0.5],  # Assuming single channel for black and white images\n",
        "                                 std=[0.5])\n",
        "\n",
        "# Combine the transformations into a single Compose object\n",
        "transform = transforms.Compose([\n",
        "    resize_transform,\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])"
      ],
      "metadata": {
        "id": "vHi4JXixJkad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = CustomXrayDataset(labels=val_labels, paths=val_paths, transform=transform)\n",
        "test_dataset = CustomXrayDataset(labels=test_labels, paths=test_paths, transform=transform)"
      ],
      "metadata": {
        "id": "w5hFrv8PJlge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "Yv1VC49SJmY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "0NR_Y15ZKQiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # Max pooling layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 512)  # Adjusted input size after two max-pooling layers\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 32)\n",
        "        self.fc4 = nn.Linear(32, 1)\n",
        "\n",
        "        # Dropout layer to prevent overfitting\n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "\n",
        "        # ReLU activation function\n",
        "        self.relu = nn.ReLU()\n",
        "        self.prob_func = nn.Sigmoid()\n",
        "\n",
        "    def weight_init(self, mean, std):\n",
        "      for m in self._modules:\n",
        "        if isinstance(self._modules[m], nn.ConvTranspose2d) or isinstance(self._modules[m], nn.Conv2d):\n",
        "          self._modules[m].weight.data.normal_(mean, std)\n",
        "          self._modules[m].bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
        "        # Flatten the output for the fully connected layers\n",
        "        x = x.view(-1, 64 * 8 * 8)  # Adjusted size after two max-pooling layers\n",
        "\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        x = self.dropout(self.relu(self.fc2(x)))\n",
        "        x = self.dropout(self.relu(self.fc3(x)))\n",
        "        x = self.prob_func(self.fc4(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "net.train()\n"
      ],
      "metadata": {
        "id": "595F985vKSsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop\n"
      ],
      "metadata": {
        "id": "EQnt0i8KKbCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Initialize weights\n",
        "net.weight_init(mean=0.0, std=0.02)\n",
        "\n",
        "# Initialize scheduler\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)"
      ],
      "metadata": {
        "id": "VGgcpdLLKaZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tables to plot learning curves\n",
        "train_losses = []\n",
        "val_losses = []"
      ],
      "metadata": {
        "id": "FKU3MSdSKdyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Initialize weights\n",
        "net.weight_init(mean=0.0, std=0.02)\n",
        "\n",
        "# Initialize scheduler\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)"
      ],
      "metadata": {
        "id": "E1Anyc2TKhPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "2vplE-EXLFD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your model to evaluation mode\n",
        "net.eval()\n",
        "device = 'cuda'\n",
        "# Initialize lists to store predictions and true labels\n",
        "all_predictions = []\n",
        "all_targets = []\n",
        "\n",
        "# Iterate over the validation dataloader\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (inputs, targets) in enumerate(test_dataloader):\n",
        "      # Move inputs and targets to the selected device\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # Convert outputs to binary predictions (0 or 1)\n",
        "      predicted_classes = torch.round(outputs).squeeze().cpu().detach().numpy()\n",
        "\n",
        "      # Append predictions and targets to the lists\n",
        "      all_predictions.extend(predicted_classes)\n",
        "      all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_targets = np.array(all_targets)\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(all_targets, all_predictions)\n",
        "\n",
        "# Print the classification report\n",
        "print(report)\n",
        "\n",
        "net.train()"
      ],
      "metadata": {
        "id": "_s79ThMeLGmc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}